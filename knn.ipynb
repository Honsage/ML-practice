{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e819682",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b575e4b",
   "metadata": {},
   "source": [
    "## Computational Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8baba8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6e42e",
   "metadata": {},
   "source": [
    "### Euclidean Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63589661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(p, q, dim):\n",
    "    dist = 0\n",
    "    for x in range(dim):\n",
    "        dist += pow(p[x] - q[x], 2)\n",
    "    return math.sqrt(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c885d",
   "metadata": {},
   "source": [
    "### Determine the Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98522494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbours(p, scope, k):\n",
    "    distances = []\n",
    "    dim = len(p) - 1\n",
    "    for q in scope:\n",
    "        dist = euclidean_dist(p, q, dim)\n",
    "        distances.append((q, dist))\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    neighbours = map(lambda x: x[0], distances[:k])\n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f8ccee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(neighbours):\n",
    "    votes = {}\n",
    "    for vote in map(lambda x: x[-1], neighbours):\n",
    "        votes[vote] = votes.get(vote, 0) + 1\n",
    "    sorted_votes = sorted(votes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_votes[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f8362",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce7b1615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(test, pred):\n",
    "    correct = sum(1 for i in range(len(test)) if test[i] == pred[i])\n",
    "    return (correct / float(len(test))) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b1d8a",
   "metadata": {},
   "source": [
    "## Implementation on Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45147b00",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb28224d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     fractal_dimension_mean  ...  texture_worst  perimeter_worst  area_worst  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  labels  \n",
       "0                  0.2654          0.4601                  0.11890       1  \n",
       "1                  0.1860          0.2750                  0.08902       1  \n",
       "2                  0.2430          0.3613                  0.08758       1  \n",
       "3                  0.2575          0.6638                  0.17300       1  \n",
       "4                  0.1625          0.2364                  0.07678       1  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       1  \n",
       "565                0.1628          0.2572                  0.06637       1  \n",
       "566                0.1418          0.2218                  0.07820       1  \n",
       "567                0.2650          0.4087                  0.12400       1  \n",
       "568                0.0000          0.2871                  0.07039       2  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/knn-data.csv')\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 2})\n",
    "labels = df['diagnosis'].tolist()\n",
    "df['labels'] = labels\n",
    "df = df.drop(['id', 'Unnamed: 32', 'diagnosis'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a87a2",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03c11770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, test number: 395 174\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "mask = np.random.rand(len(df)) < 0.7\n",
    "train, test = df[mask], df[~mask]\n",
    "print(\"Train, test number:\", len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82b0ad",
   "metadata": {},
   "source": [
    "### Calculating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "389fa400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=2.0\n",
      "> predicted=1.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=2.0, actual=2.0\n",
      "> predicted=1.0, actual=1.0\n",
      "> predicted=1.0, actual=1.0\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "\n",
    "pred = []\n",
    "train_list = train.values.tolist()\n",
    "test_list = test.values.tolist()\n",
    "\n",
    "for p in test_list:\n",
    "    neighbours = get_neighbours(p, train_list, k)\n",
    "    result = get_response(neighbours)\n",
    "    pred.append(result)\n",
    "    print('> predicted=' + repr(result) + ', actual=' + repr(p[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1d46d6",
   "metadata": {},
   "source": [
    "### Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72bb1d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 52   5]\n",
      " [  6 111]]\n",
      "Accuracy: 93.67816091954023 , Custom Accuracy: 93.67816091954023\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "y_test = list(map(lambda x: x[-1], test_list))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred) * 100, \", Custom Accuracy:\", calc_accuracy(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d4680",
   "metadata": {},
   "source": [
    "## KNN with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9bf30d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 0.942\n",
      "\n",
      "Отчет по классификации:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.90      0.92        63\n",
      "           2       0.95      0.96      0.95       108\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.94      0.93      0.94       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "X = np.array(df.drop(['labels'], axis=1))\n",
    "y = np.array(df['labels'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Точность: {accuracy:.3f}\")\n",
    "print(\"\\nОтчет по классификации:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "772ce4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(cm, cmap='Blues')\n",
    "\n",
    "plt.xticks([0, 1], ['Predicted 0', 'Predicted 1'])\n",
    "plt.yticks([0, 1], ['Actual 0', 'Actual 1'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, str(cm[i, j]), \n",
    "                ha='center', va='center', \n",
    "                color='white' if cm[i, j] > 50 else 'black')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
